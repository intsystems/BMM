# Distributions
* [Heavy tails in Bayesian NN](http://bayesiandeeplearning.org/2018/papers/64.pdf) [reporter: Marat Khusainov]
* ~~[Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks](https://arxiv.org/pdf/1810.06767.pdf) [reporter: "your name"]~~

# Bayesian inference
* ~~[Hyperparameters: optimize, or integrate out?](http://www.inference.org.uk/mackay/abstracts/alpha.html) [reporter: "your name"]~~
* [Learning Approximately Objective Priors](https://arxiv.org/pdf/1704.01168.pdf) [reporter: Galina Boeva]

# Model complexity
* [A widely applicable Bayesian information criterion](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) [reporter: "your name"]
* ~~[The Description Length of Deep Learning Models](https://proceedings.neurips.cc/paper/2018/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf)~~  [reporter: "Gavrilyuk Alexander"]

# Variational inference 1
~~* Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) [reporter: "your name"]~~
* [Rényi divergence](https://proceedings.neurips.cc/paper_files/paper/2016/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf) [reporter: Kseniia Petrushina]

# Variationl inference 2
* [Variational dropout](https://proceedings.neurips.cc/paper/2015/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf) [reporter: Eduard Vladimirov]
* [Alpha-divergence](http://proceedings.mlr.press/v48/hernandez-lobatob16.pdf)  [reporter: Boeva Galina]

# Graphical models
* [Learning to Discover Sparse Graphical Models](http://proceedings.mlr.press/v70/belilovsky17a/belilovsky17a.pdf) [reporter: "your name"]
* [Probabilistic circuits](http://starai.cs.ucla.edu/papers/ProbCirc20.pdf) [reporter: "Parviz Karimov"]

# Generative vs. Discriminative
* [Hypertransformer: Model generation for supervised and semi-supervised few-shot learning](https://proceedings.mlr.press/v162/zhmoginov22a/zhmoginov22a.pdf) [reporter: "Timofey Chernikov"]
* [OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://proceedings.neurips.cc/paper/2019/file/0ae775a8cb3b499ad1fca944e6f5c836-Paper.pdf) [reporter: Kseniia Petrushina]

# Task 1 discussion (optional talk)
* [Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks](https://arxiv.org/pdf/1810.06767.pdf) [reporter: "your name"]
* [Hyperparameters: optimize, or integrate out?](http://www.inference.org.uk/mackay/abstracts/alpha.html) [reporter: "your name"]
* [A widely applicable Bayesian information criterion](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) [reporter: "your name"]
* Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) [reporter: Maksim Tyurikov]

# Generative models
* [Dangers of Bayesian Model Averaging under Covariate Shift](https://arxiv.org/pdf/2106.11905.pdf) [reporter: "Timofey Chernikov"]
* [Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://proceedings.neurips.cc/paper_files/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf) [reporter: "Dmitry Protasov"]

# Hyperparameter optimization
* [Weighted Random Search for Hyperparameter Optimization](https://arxiv.org/pdf/2004.01628.pdf) [reporter: Boeva Galina]
* [c-TPE: Tree-Structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization](https://arxiv.org/pdf/2211.14411.pdf) [reporter: "your name"]

# Gradient-based hyperparameter optimization
* [Bayesian Optimization with Gradients](https://proceedings.neurips.cc/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Paper.pdf)  [reporter: "your name"]
* [Forward and Reverse Gradient-Based Hyperparameter Optimization](https://arxiv.org/pdf/1703.01785.pdf)   [reporter: Maksim Tyurikov]

# Task 2 discussion (optional talk)
* [Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks](https://arxiv.org/pdf/1810.06767.pdf) [reporter: "your name"]
* [Hyperparameters: optimize, or integrate out?](http://www.inference.org.uk/mackay/abstracts/alpha.html) [reporter: "your name"]
* [A widely applicable Bayesian information criterion](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) [reporter: "your name"]
* [c-TPE: Tree-Structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization](https://arxiv.org/pdf/2211.14411.pdf) [reporter: "your name"]
* [Bayesian Optimization with Gradients](https://proceedings.neurips.cc/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Paper.pdf)  [reporter: "your name"]

# Structure selection
*  Neural Architecture Search without Training [article](https://arxiv.org/abs/2006.04647) [reporter: Dmitry Protasov]
*  Bayesnas: A bayesian approach for neural architecture search  [article](http://proceedings.mlr.press/v97/zhou19e/zhou19e.pdf) [reporter: "your name"]
*  Bananas: Bayesian optimization with neural architectures for neural architecture search [article](https://ojs.aaai.org/index.php/AAAI/article/download/17233/17040) [reporter: Boeva Galina]

# Random processes and genetics for model generation
* [AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](http://proceedings.mlr.press/v119/real20a/real20a.pdf) [reporter: Kseniia Petrushina]
* [Proving the Lottery Ticket Hypothesis: Pruning is All You Need](http://proceedings.mlr.press/v119/malach20a/malach20a.pdf) [reporter: "your name"]


# Meta-optimization
* [Generalized Inner Loop Meta-Learning](https://arxiv.org/pdf/1910.01727.pdf) [reporter: "your name"]
* [HOW TO TRAIN YOUR MAML](https://arxiv.org/pdf/1810.09502.pdf)  [reporter: Dmitry Protasov]


# Multi-task learning
* [Discovering Inductive Bias with Gibbs Priors: A Diagnostic Tool for Approximate Bayesian Inference](https://proceedings.mlr.press/v151/rendsburg22a/rendsburg22a.pdf) [reporter: "your name"]
* [Variational multi-task learning with gumbel-softmax priors](https://proceedings.neurips.cc/paper_files/paper/2021/file/afd4836712c5e77550897e25711e1d96-Paper.pdf) [reporter: "your name"]

# Knowledge transfer
* [Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf)  [reporter: Boeva Galina]
* [Learning to select data for transfer learning with bayesian optimization](https://arxiv.org/pdf/0907.1815.pdf)  [reporter: "your name"]
* [Knowledge transfer via dense cross-layer mutual-distillation](https://arxiv.org/pdf/2008.07816)   [reporter: "your name"]

# Sampling
* [Data augmentation in Bayesian neural networks and the cold posterior effect](https://proceedings.mlr.press/v180/nabarro22a/nabarro22a.pdf) [reporter: "Marat Khusainov"]
* [Evolutionary MCMC](https://cdn.aaai.org/ICML/2003/ICML03-096.pdf) [reporter: "your name"]


# Probabilistic metric spaces
* [AN INDUCTIVE BIAS FOR DISTANCES: NEURAL NETS THAT RESPECT THE TRIANGLE INEQUALITY](https://arxiv.org/pdf/2002.05825.pdf)  [reporter: "your name"]
* [MsC: Siamese networks + prob. metric learning](https://tspace.library.utoronto.ca/bitstream/1807/43097/3/Liu_Chen_201311_MASc_thesis.pdf)   [reporter: "your name"]

# Projection into latent space
* [Neural operator search](https://www.sciencedirect.com/science/article/pii/S003132032200694X) [reporter: "your name"]
* [Super resolution neural operator](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Super-Resolution_Neural_Operator_CVPR_2023_paper.pdf) [reporter: "your name"]

# Model ensembles
* [Functional MOE](https://link.springer.com/article/10.1007/s11222-023-10379-0) [reporter: "your name"]
* [Neural ensemble search via Bayesian sampling](https://proceedings.mlr.press/v180/shu22a/shu22a.pdf) [reporter: Kseniia Petrushina]

# Gaussian processes
* [The Variational Gaussian Process](https://arxiv.org/abs/1511.06499) [reporter: Dmitry Protasov]
* [State Space Expectation Propagation: Efficient Inference Schemes for Temporal Gaussian Processes](https://arxiv.org/abs/2007.05994) [reporter: "your name"]
