# Distributions
* [Heavy tails in Bayesian NN](http://bayesiandeeplearning.org/2018/papers/64.pdf) [reporter: Marat Khusainov]
* ~~[Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks](https://arxiv.org/pdf/1810.06767.pdf) [reporter: "your name"]~~

# Bayesian inference
* ~~[Hyperparameters: optimize, or integrate out?](http://www.inference.org.uk/mackay/abstracts/alpha.html) [reporter: "your name"]~~
* [Learning Approximately Objective Priors](https://arxiv.org/pdf/1704.01168.pdf) [reporter: Galina Boeva]

# Model complexity
* [A widely applicable Bayesian information criterion](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) [reporter: "your name"]
* ~~[The Description Length of Deep Learning Models](https://proceedings.neurips.cc/paper/2018/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf)~~  [reporter: "Gavrilyuk Alexander"]

# Variational inference 1
~~* Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) [reporter: "your name"]~~
* [Rényi divergence](https://proceedings.neurips.cc/paper_files/paper/2016/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf) [reporter: Kseniia Petrushina]

# Variationl inference 2
* [Variational dropout](https://proceedings.neurips.cc/paper/2015/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf) [reporter: Eduard Vladimirov]
* [Alpha-divergence](http://proceedings.mlr.press/v48/hernandez-lobatob16.pdf)  [reporter: Boeva Galina]

# Graphical models
* [Learning to Discover Sparse Graphical Models](http://proceedings.mlr.press/v70/belilovsky17a/belilovsky17a.pdf) [reporter: "your name"]
* [Probabilistic circuits](http://starai.cs.ucla.edu/papers/ProbCirc20.pdf) [reporter: "Parviz Karimov"]

# Generative vs. Discriminative
* [Hypertransformer: Model generation for supervised and semi-supervised few-shot learning](https://proceedings.mlr.press/v162/zhmoginov22a/zhmoginov22a.pdf) [reporter: "Timofey Chernikov"]
* [OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://proceedings.neurips.cc/paper/2019/file/0ae775a8cb3b499ad1fca944e6f5c836-Paper.pdf) [reporter: Kseniia Petrushina]

# Task 1 discussion (optional talk)
* [Approximate Fisher Information Matrix to Characterise the Training of Deep Neural Networks](https://arxiv.org/pdf/1810.06767.pdf) [reporter: "your name"]
* [Hyperparameters: optimize, or integrate out?](http://www.inference.org.uk/mackay/abstracts/alpha.html) [reporter: "your name"]
* [A widely applicable Bayesian information criterion](https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf) [reporter: "your name"]
* Scalable marginal likelihood estimation for model selection in deep learning [article](http://proceedings.mlr.press/v139/immer21a/immer21a.pdf) [reporter: Maksim Tyurikov]

# Generative models
* [Dangers of Bayesian Model Averaging under Covariate Shift](https://arxiv.org/pdf/2106.11905.pdf) [reporter: "Timofey Chernikov"]
* [Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://proceedings.neurips.cc/paper_files/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf) [reporter: "Dmitry Protasov"]
