Deadline: October 31, 23.59.

Save notebooks into task1/SurnameTask1.ipynb

**IMPORTANT:** the code must not be written in Torch/Tensorflow. For deep learning use Jax.


1.  [Reporter: TODO]  Compare Kronecker Laplace, and standard Laplace approximation methods (in 3 variants: with full covariance, diagonal covariance, scalar covariance) for a logistic regression on a synthetic dataset generated with
 parameters generated by Gaussian distribution with full-rank covariance. The hyperparameters must be optimized. Estimate: the model performance, approximation quality, the quality of covariance restoration, speed of the method with different dataset size and dimensionality.
* [hyperparameter optimization, Laplace method](http://strijov.com/papers/HyperOptimizationEng.pdf)
* [Kronecker Laplace](https://discovery.ucl.ac.uk/id/eprint/10080902/1/kflaplace.pdf)
  
2.  [Reporter: TODO]  Compare ELBO and standard Laplace approximation methods (in 3 variants: with full covariance, diagonal covariance, scalar covariance) for a linear regression on a synthetic dataset  parameters generated by Gaussian distribution with full-rank covariance.
Estimate quality of approximation, speed of approximation, and KL divergence between approximatng distribution and posterior distribution.
  * [hyperparameter optimization, Laplace method](http://strijov.com/papers/HyperOptimizationEng.pdf)
  * [ELBO](https://www.cs.toronto.edu/~graves/nips_2011.pdf)

3.  [Reporter: TODO]  Analyze a naive method of hyperparameter optimization from Graves 2011. Adapt it to a full and diagonal covariance matrix. Estimate covariance estimation quality for datasets generated with different covariance matrix types.
* [ELBO](https://www.cs.toronto.edu/~graves/nips_2011.pdf)
  

4.  [Reporter: Boeva Galina]  Run ELBO with different parameter sampling strategy for a neural network (at least 3 layers MLP). Dataset: MNIST (or more complex dataset). Parameter sampling strategies:
    - One sample per batch (see Graves, 2011)
    - One sample per batch element (naively)
    -  One sample per batch element using local reparametrization
  * [ELBO](https://www.cs.toronto.edu/~graves/nips_2011.pdf)
  * [Local reparametrization](https://arxiv.org/abs/1506.02557)

5.  [Reporter: TODO]  Compare approximation for a Gaussian distribution and a Gaussian mixture using SGD, SGLD and Stein gradient descent. For each method optimize hyperparameters and show dependence of the quality from the iteration number.
  * [Stein operator](https://proceedings.neurips.cc/paper_files/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf)
  * [SGLD](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)
  * [SGD as ELBO](https://arxiv.org/abs/1504.01344)  

6.  [Reporter: TODO]  Compute and compare ELBO for a model with parameters distributed using Gaussian mixture. The ELBO must be computed using a simple ELBO (Graves) and using implicit reparametrization trick.
       * [ELBO](https://www.cs.toronto.edu/~graves/nips_2011.pdf)
       * [Implicit reparametrization](https://proceedings.neurips.cc/paper_files/paper/2018/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf)

   
7. [Reporter: TODO] Repeat plot 28.6 from MacKay book. Dataset: sklearn dataset. 
    Prior: normal distribution with scalar covariance. Models: multiple linear regression models:
    1. with optimal hyperparameters
    2. with lowered variance for the prior 
    3. with biased mean for the prior
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
* [book](http://www.inference.org.uk/itprnn/book.pdf)
* [hyperparameter optimization](http://strijov.com/papers/HyperOptimizationEng.pdf)
* Visualization: [help1](https://matplotlib.org/stable/users/interactive.html),[help2](https://stackoverflow.com/questions/44329068/jupyter-notebook-interactive-plot-with-widgets),[help3](https://towardsdatascience.com/matplotlib-animations-in-jupyter-notebook-4422e4f0e389)

